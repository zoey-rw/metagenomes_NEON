# Downloading shotgun metagenomes from NEON

In this brief tutorial, we will download raw shotgun metagenomics sequence data produced by the National Ecological Observatory Network (NEON). Once downloaded, we will decompress files and fix file names.

```{r setup, include = FALSE}
knitr::opts_chunk$set(eval = FALSE) # change this to TRUE for the code to run 
```

First, determine which sites or months have data that interest you. You can view a map of sites here: https://www.neonscience.org/field-sites/field-sites-map
In the code snippet below, we access all available data for the NEON site in Soaproot Saddle, CA, which has the site ID "SOAP". We use the `neonUtilities` R package to retrieve metadata that includes URLs for raw sequence files. 
```{r}
#install.packages('neonUtilities') # install from CRAN if necessary
library(neonUtilities) 
metadata <- loadByProduct(dpID = 'DP1.10107.001',
                          startdate = "2016-09", enddate = "2016-09",
                          check.size = FALSE, package = 'expanded')
#metadata <- readRDS("/projectnb/talbot-lab-data/zrwerbin/metagenomes_raw/all_metagenome_metadata.rds")
# We will use the rawDataFiles table to get download URLs
rawFileInfo <- metadata$mms_rawDataFiles 
```
We first check to see if metagenome files are already downloaded, and remove these files from our dataframe. Downloaded files have various portions of the extension .fastq.tar.gz, which tells us about the contents: fastq refers to the sequence file format, tar refers to the archive of multiple sequence files, and gz refers to the compression of this archive. Each tar.gz file can have multiple samples included, so we only want the unique file paths.
```{r}
download.dir <- paste0(getwd(), "/raw_metagenomes") 
already.have <- list.files(download.dir, pattern = ".fastq", recursive = T)
already.have <- gsub("_R[12].fastq|_R[12].fastq.gz", "", already.have)
already.have <- basename(already.have)
not.have <- rawFileInfo[!rawFileInfo$dnaSampleID %in% already.have,]
not.have <- not.have[!not.have$internalLabID %in% already.have,]
url_list <- unique(not.have$rawDataFilePath)

#only taking the first URL for example purposes - remove this line for real downloads
url_list <- url_list[1]
```

Next, we use the lapply function to download files associated with each URL. These will download to your working directory unless you change the file path below. 
```{r} 
lapply(1:length(url_list), function(i) {
  download.file(url_list[i],  destfile = paste0(download.dir, basename(url_list[i])))
})
```

Here we use command line tools to "untar" each of the downloaded files, and remove the original tar files.
```{r}
cmd <- paste0('cd ', download.dir, ';', ' for file in *.tar.gz; do tar xzvf "${file}" && rm "${file}"; done')
system(cmd)
```

Some of the resulting files are nested within many directories - let's bring everything to the main directory, and then remove any empty directories.
```{r}
cmd <- paste0('find ', download.dir, ' -mindepth 2 -type f -exec mv -t ', download.dir, ' -i "{}" +')
system(cmd)
cmd <- paste0('find ', download.dir, '/. -type d -empty -delete')
system(cmd)
```

Finally, let's rename the samples according to their NEON sampleIDs, since the current files were named by the sequencing facility. Some files have not been renamed, if they weren't in the metadata file, since the tar files are bundled samples from multiple sites. 
```{r}
setwd(download.dir)
suffix <- ifelse(grepl("R2_", rawFileInfo$rawDataFileName), "_R2", "_R1")
# Add "BMI" prefix to metagenomes if necessary
old.names <- ifelse(!grepl("BMI", rawFileInfo$internalLabID), paste0("BMI_", rawFileInfo$internalLabID, suffix, ".fastq"), paste0(rawFileInfo$internalLabID, suffix, ".fastq"))
new.names <- paste0(rawFileInfo$dnaSampleID, suffix, ".fastq")
file.rename(from = old.names, to = new.names)
```

We'll put the renamed files into their own folder.
```{r}
cleaned <- list.files(download.dir, pattern = "COMP-DNA")
dir.create("clean")
file.rename(cleaned, paste0("clean/", cleaned))
```

Remove unpaired files.
```{r}
actual.files <- list.files(file.path(download.dir, "clean"), full.names = T)
basenames <- sapply(strsplit(actual.files, "_R"), `[`, 1)
to.rm <- actual.files[!basenames %in% basenames[duplicated(basenames)]]
file.remove(to.rm)
cat(paste(to.rm, "does not have an read counterpart. Omitting from this analysis.\n"))
```

Finally, you must compress all files so that they have a 'gz' extension - Sunbeam expects all input files to be zipped. `gzip` is much slower than `pigz`, but the `pigz` software may not be available on your system. On a shared cluster, running `module load pigz` on the command line might make the software accessible.
```{r}
# cmd <- paste0("gzip ", file.path(download.dir, "clean"), "/*.fastq") # this compresses the files (required for Sunbeam)
# system(cmd)
```

